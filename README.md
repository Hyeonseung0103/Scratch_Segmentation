# 차량 이미지 내의 스크래치 위치 파악

차량 이미지 내의 스크래치 위치 파악을 위한 Image Segmentation 모델 개발이라는 주제로 진행한 프로젝트이다. 넥스트랩 기업과 연계하여 프로젝트를 개인으로 진행했고, 이미지 라벨링, 데이터 수집, 데이터 전처리, 모델링, 시각화 등의 역할을 수행했다.


## 프로젝트 개요 및 필요성

- 오늘날에는 쏘카, 그린카와 같은 어플의 등장으로 렌트카 서비스가 활발하게 이루어지고 있다. 대표적인 렌트카 어플 중 하나인 쏘카에서는 하루에 약 10만장의 차량 이미지 사진이 업로드 된다고 하는데 차량의 손상 여부를 확인하기 위해 사람이 이 모든 사진을 눈으로 검토하기에는 많은 시간과 비용이 요구된다. 

- 본 프로젝트는 이러한 문제를 해결하기 위해 차량의 손상 중 하나인 스크래치에 대해 집중적으로 학습하면서 차량 이미지에서 스크래치 위치를 검출해내는 Image Segmentation 모델을 개발했다.

- 모델이 스크래치의 유무와 위치를 분류한다면 차량 이미지를 검토하는 인원이 일일이 모든 사진을 확인해야 하는 번거로움이 줄어든다는 것을 해당 프로젝트의 기대효과라고 할 수 있다.

## 프로젝트 파이프라인

1. 데이터는 넥스트랩 기업과 AI-Hub에서 제공. 

2. 제공 받은 데이터는 파이썬과 Open CV 라이브러리를 활용해 전처리(256 x 256으로 크기 통일, 색상 조정, 조명과 빛 반사 제거를 위한 광원 제거, 마스킹 이미지 생성 등).

3. 딥러닝 프레임워크 중 하나인 Pytorch를 기반으로 U-Net, U-Net3Plus, DeepLabV3 모델을 개발. 

4. 모델의 예측 결과는 파이썬의 matplotlib 라이브러리를 활용하여 시각화.

![image](https://user-images.githubusercontent.com/97672187/183606324-7ea14e26-893a-4528-a0d0-e88f98953549.png)

## 데이터 설명

- 수집 및 모델에 사용한 데이터: 넥스트랩 기업 제공(3,000) + AI-Hub 사이트 제공(13,000) = 약 16,000개

- 학습 데이터: 13,000개

- 검증 및 테스트 데이터: 3,000개(학습할 데이터가 충분하지 않다고 판단하여 검증 데이터를 테스트 데이터로도 활용)

- 데이터 용량: 약 1GB

- 원본 이미지 / 마스킹(라벨 이미지)

![image](https://user-images.githubusercontent.com/97672187/183614179-e88575d2-2cfb-4f9d-9043-96bb423b71ad.png)

- 원본 / 흑백/ 광원제거

![image](https://user-images.githubusercontent.com/97672187/183614333-50eb308f-2afe-4cdd-9886-5962a74d488b.png)

## 모델링
손실함수는 주로 Dice Loss를 사용했고, Optimizer는 Adam, 평가지표는 IoU Score를 사용했다.

1. U-Net

- Segmentation 모델 중 하나로 이전 층의 공간 정보를 최대한 보존하면서 이미지를 픽셀 단위로 분류시키는 모델.

- Encoding path에서의 Downsampling(채널의 수는 늘리고, 피처맵의 크기는 줄어듬)과정에서 만들어진 피처맵의 일부를 Decoding path의 Upsampling(채널의 수는 줄고, 피처맵의 크기는
커짐)과정에서 사용함으로써 이전 층의 공간 정보 손실을 최소화 시키려는 것이 특징이다.

2. U-Net3Plus

3. DeepLabV3

이렇게 모델 설명 간단하게 하고, 배달 프로젝트처럼 1차, 2차, 3차 모델링 느낌으로 성능 비교하자. ppt에 성능은 다 나와있으니까 그거 쓰면 될 듯.



## 결론

- 예측 결과 시각화

![image](https://user-images.githubusercontent.com/97672187/183616510-42e7285b-82e1-44ea-8235-357db2788761.png)

위의 두 사진은 원본 이미지와 라벨 이미지고, 그 옆에 오른쪽에 있는 사진은 손실함수로 Binary Cross Entropy를 사용했을 때의 U-Net 모델의 예측 결과, 밑의 사진은 손실함수로 Dice Loss를 사용했을 때 모델들의 예측 결과이다.

- 초기에는 손실함수로 Binary Cross Entropy를 사용했는데 불균형 데이터의 특성상 모델이 모든 픽셀을 0의 클래스로 즉, 스크래치가 없는 픽셀로 분류를 하더라도 손실이 매우 낮게 나온다는 문제점이 있다. 해당 이미지에서는 0의 클래스가 약 97퍼센트 이상이기 때문에 모델이 모든 픽셀을 배경으로 예측 해도 매우 높은 정확도를 가지는 것처럼 학습하게 된다는 것이다. 

- 따라서 배경인 부분보다 스크래치가 난 부분에 초점을 둔 Dice Loss를 손실함수로 사용했다. 

- 밑의 사진의 모델들은 모두 다른 형태로 스크래치를 분류했는데 밑에서 4번째 그림인 최고 성능을 낸 U-Net 모델이 가장 비슷한 예측을 한 것을 알 수 있다. 광원을 제거한 데이터를 학습시킨 모델은 다른 U-Net 모델들에 비해 부정확한 예측을 한 것을 볼 수 있는데 광원을 제거하면서 스크래치가 난 부분 또한 흐릿하게 되어버린 것이 원인이 된 것으로 판단된다.

- DeepLabV3은 해당 이미지 외의 다른 이미지도 U-Net에 비해 예측률이 떨어져서 더 학습시키진 않았다.

- 추가로, 에포크를 10만 주고 학습했던 U-Net3Plus 모델도 비슷한 형태로 스크래치를 분류한 것을 확인할 수 있다.

## 한계점 및 해결방안

1. 스크래치 외의 손상에 대해서는 분류하지 못했다. 

-> 스크래치 외의 손상 데이터를 제공받는다면 찍힘, 벌어짐 등과 같은 다른 손상들도 다중 분류 문제로 해결할 수 있을 것 같다. 

2. 데이터 양이 적어서 과적합을 해결하기 어려웠다.

3. 상위 모델인 U-Net3Plus 모델을 더 깊게 학습시키지 못했다. 

-> 위 2가지 문제는 더 많은 시간과 좋은 GPU 환경이 주어진다면 훨씬 더 많은 데이터를 사용하고, 성능이 좋은 U-Net3Plus 모델을 더 깊게 학습시킴으로써 지금보다 더 좋은 모델을 만들 수 있을 것 같다.

## 개발환경

![image](https://user-images.githubusercontent.com/97672187/183615283-dcb227d4-1cdc-4ab9-baf4-e2567c44d5f9.png)

